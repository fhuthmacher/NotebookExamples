{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "864c99c5-5ef3-4111-a3a3-d109fee5f602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "load_dotenv(find_dotenv('dev.env'),override=True)\n",
    "\n",
    "os.environ['LANGCHAIN_ASSUME_ROLE'] = os.getenv('LANGCHAIN_ASSUME_ROLE')\n",
    "os.environ['BEDROCK_REGION_NAME'] = os.getenv('BEDROCK_REGION_NAME')\n",
    "os.environ['BEDROCK_ENDPOINT_URL'] = os.getenv('BEDROCK_ENDPOINT_URL')\n",
    "os.environ['OPENSEARCH_COLLECTION'] = os.getenv('OPENSEARCH_COLLECTION')\n",
    "os.environ['OPENSEARCH_REGION'] = os.getenv('OPENSEARCH_REGION')\n",
    "\n",
    "# Initialize the Bedrock runtime\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 8\n",
    "   }\n",
    ")\n",
    "bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=os.environ.get(\"BEDROCK_REGION_NAME\", None),\n",
    "        config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296a13d4-2af0-445b-b000-cbdf3e33048d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pypdf\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [ \"https://d3q8adh3y5sxpk.cloudfront.net/meetingrecordings/modelevaluationdata/amazon_10k_2023.pdf\"]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)\n",
    "    \n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c986f99-9175-4a66-8f24-61509b23c719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter split documents in to 165 chunks.\n",
      "\n",
      "CharacterTextSplitter split documents in to 651 chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Split documents into smaller chunks \n",
    "### Compare results/impact of Character split and TokenTextSplitter\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "token_text_list = token_text_splitter.split_documents(documents)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "    \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "print(\"CharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "887cafda-73bf-4212-ae0b-b82946882e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create vectors and store them in our vector database (OpenSearch Serverless)\n",
    "### Connect to OpenSearchServerless\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION']  # serverless collection endpoint, without https://\n",
    "region = os.environ['OPENSEARCH_REGION']  # e.g. us-east-1\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dca2f1be-d902-4360-b800-61f972d8c790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating index 'llmqageneration' on OpenSearch.\n"
     ]
    }
   ],
   "source": [
    "### Create a index in Amazon Opensearch Service \n",
    "\n",
    "# langchain version\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"llmqageneration\"\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on OpenSearch.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "    aos_client.indices.get(index=index_name)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on OpenSearch.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "    aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deafa090-496a-40c0-8062-f7f94cb0c7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llmqageneration': {'aliases': {},\n",
       "  'mappings': {'properties': {'text': {'type': 'text', 'store': True},\n",
       "    'vector_field': {'type': 'knn_vector', 'store': True, 'dimension': 1536}}},\n",
       "  'settings': {'index': {'number_of_shards': '2',\n",
       "    'provided_name': 'llmqageneration',\n",
       "    'knn': 'true',\n",
       "    'creation_date': '1696180007445',\n",
       "    'number_of_replicas': '0',\n",
       "    'uuid': 'GBI07IoBlXWWdUA78dGU',\n",
       "    'version': {'created': '135217827'}}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"llmqageneration\"\n",
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d76ffb5-2505-4fde-98c7-93ce16989959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 108, 'timed_out': False, '_shards': {'total': 0, 'successful': 0, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 11, 'relation': 'eq'}, 'max_score': 0.0018039581, 'hits': [{'_shard': '[026459568683::89rogzwnoq45nomc0tvh::VECTORSEARCH::llmqageneration:0][1]', '_node': '10.0.153.86', '_index': 'llmqageneration', '_id': '1%3A0%3AATI17IoBdWq38RY9sq9j', '_score': 0.0018039581, '_source': {'text': 'We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our stores to enable hundreds of\\nmillions of unique products to be sold by us and by third parties across dozens of product categories. Customers access our offerings through our websites,\\nmobile apps, Alexa, devices, streaming, and physically visiting our stores. We also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire'}, 'fields': {'text': ['We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our stores to enable hundreds of\\nmillions of unique products to be sold by us and by third parties across dozens of product categories. Customers access our offerings through our websites,\\nmobile apps, Alexa, devices, streaming, and physically visiting our stores. We also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire']}, '_explanation': {'value': 1.0, 'description': 'No Explanation', 'details': []}}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'We serve consumers through our online and physical stores and focus on selection, price, and convenience. We design our stores to enable hundreds of\\nmillions of unique products to be sold by us and by third parties across dozens of product categories. Customers access our offerings through our websites,\\nmobile apps, Alexa, devices, streaming, and physically visiting our stores. We also manufacture and sell electronic devices, including Kindle, Fire tablet, Fire'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Insert embeddings into OpenSearch\n",
    "\n",
    "def get_embedding(body, modelId, accept, contentType):\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    return embedding\n",
    "\n",
    "def embed_phrase(phrase):\n",
    "    body = json.dumps({\"inputText\": str(phrase)})\n",
    "    modelId = 'amazon.titan-embed-text-v1' #'amazon.titan-e1t-medium' # not available yet\n",
    "    contentType = 'application/json'\n",
    "    accept = 'application/json'\n",
    "    embedding = get_embedding(body, modelId, accept, contentType)\n",
    "    return embedding\n",
    "    \n",
    "def os_import(record, aos_client, index_name):\n",
    "    search_vector = embed_phrase(record)\n",
    "    aos_client.index(index=index_name,\n",
    "             body={\"vector_field\": search_vector,\n",
    "                   \"text\": record\n",
    "                  },\n",
    "            request_timeout=60*3,  # 3 minutes\n",
    "            )\n",
    "\n",
    "def query_opensearch(index_name, phrase, n=1):\n",
    "    search_vector = embed_phrase(phrase)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"vector_field\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"vector_field\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"text\"],\n",
    "                           explain = True)\n",
    "    print(res)\n",
    "    result = {\n",
    "            \"text\":\"\"\n",
    "        }\n",
    "    if res['hits']['hits']:\n",
    "        top_result = res['hits']['hits'][0]\n",
    "    \n",
    "        result = {\n",
    "            \"text\":top_result['_source']['text'],\n",
    "\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "### test embedding\n",
    "#embed_phrase(\"pairs well with chocolate\")\n",
    "\n",
    "# test insert\n",
    "\n",
    "#for record in char_text_list: \n",
    "    #print(record.page_content)\n",
    "#    os_import(record.page_content, aos_client, index_name)\n",
    "\n",
    "#for record in token_text_list: \n",
    "    #print(record.page_content)\n",
    "#    os_import(record.page_content, aos_client, index_name)\n",
    "    \n",
    "#for record in token_text_list: \n",
    "#    print(record.page_content)\n",
    "query_opensearch(index_name, 'Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2635cb6a-2bce-41ad-b20a-8e9f404c2d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Use Titan Embeddings Model to generate embeddings\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# LangChain requires AWS4Auth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "def get_aws4_auth():\n",
    "    region = os.environ.get(\"Region\", os.environ[\"AWS_REGION\"])\n",
    "    service = \"aoss\"\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    return AWS4Auth(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        region,\n",
    "        service,\n",
    "        session_token=credentials.token,\n",
    "    )\n",
    "aws4_auth = get_aws4_auth()\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock_runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962570bf-e13a-4c44-bf5c-3e7733d76ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Alternative use Langchain Insert embeddings into OpenSearch - had some issues,that's why I used above instead\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "    \n",
    "# get embeddings and do insert to OpenSearch.\n",
    "def opensearch_insert(source_doc_list, target_index_name):\n",
    "    doc_search=OpenSearchVectorSearch.from_documents(\n",
    "            index_name = target_index_name,\n",
    "            documents=source_doc_list,\n",
    "            embedding=bedrock_embeddings,\n",
    "            opensearch_url=full_opensearch_endpoint,\n",
    "            http_auth=auth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=60*4,\n",
    "            bulk_size=1000,\n",
    "            is_aoss=True\n",
    "        )\n",
    "    inserted_doc_length = len(source_doc_list)\n",
    "    input_doclist_name = f'{char_text_list=}'.split('=')[0]\n",
    "    print(f'ingested {inserted_doc_length} documents from source {input_doclist_name} to target {target_index_name}')\n",
    "    return doc_search\n",
    "\n",
    "doc_search=opensearch_insert(source_doc_list=char_text_list, target_index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8fb97841-f1b0-435c-94a0-bc11bd4c6bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records found: 651.\n"
     ]
    }
   ],
   "source": [
    "### validate load\n",
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f64a6520-a819-43b8-b324-687bb4bd5f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up retrieval QA chain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 8000,\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "llm = Bedrock( #create a Bedrock llm client\n",
    "    region_name=os.environ.get(\"BEDROCK_REGION_NAME\"), #sets the region name (if not the default)\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\"), #sets the endpoint URL (if necessary)\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs=inference_modifier\n",
    ")\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "\n",
    "### define VectorStore Retriever\n",
    "doc_search = OpenSearchVectorSearch(\n",
    "        index_name=index_name,\n",
    "        embedding_function=bedrock_embeddings,\n",
    "        opensearch_url=full_opensearch_endpoint,\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        is_aoss=True\n",
    "    ) \n",
    "    \n",
    "prompt_template = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        {question}\n",
    "        - Return each response in <prompt> </prompt> XML tags.\n",
    "        - Inside <promp> tag, return the question inside <question></question> XML tags.\n",
    "        - Inside <promp> tag, return the answer inside <question_answer> </question_answer> XML tags.\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the result in XML format\n",
    "        \n",
    "        \n",
    "        Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "# see also https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.as_retriever\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=doc_search.as_retriever(search_kwargs={'k': 100}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "219df49f-de4c-4258-83a3-fc583337518a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Generate several pages of question and answer pairs and return the result in XML format. Only return the answers in xml tags\"\n",
    "results = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "07ffec96-2c7a-4e06-92a8-776cb199fc33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "The context contains ~ 9245 tokens.\n"
     ]
    }
   ],
   "source": [
    "# used context\n",
    "i = 0\n",
    "str_value = ''\n",
    "print(len(results['source_documents']))\n",
    "while i < int(len(results['source_documents'])):\n",
    "    str_value = str_value + results['source_documents'][i].page_content\n",
    "    i+=1\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "token_count = len(encoding.encode(str_value))\n",
    "print(f\"The context contains ~ {token_count} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc9270d1-1da5-4d6a-a261-eb177ae030fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### parse output to csv file\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_to_xml_method1(data_str):\n",
    "    root = ET.fromstring(data_str)\n",
    "    return root\n",
    "\n",
    "# PARSE XML\n",
    "root = convert_to_xml_method1(results['result'].split(\"\\n\", 1)[1])\n",
    "\n",
    "# check if file exists\n",
    "\n",
    "# Open our existing CSV file in append mode\n",
    "# Create a file object for this file\n",
    "filename = 'qsdata.csv'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    with open(filename, 'a',encoding='utf-8') as csvfile:\n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "        for prompt in root.findall('prompt'):\n",
    "            if(prompt):\n",
    "                # EXTRACT DETAILS  \n",
    "                question = prompt.find(\"question\")\n",
    "                answer = prompt.find(\"question_answer\")\n",
    "                csv_line = [question.text, \"<question_answer>\" + answer.text + \"</question_answer>\"]\n",
    "                # ADD A NEW ROW TO CSV FILE\n",
    "                csvfile_writer.writerow(csv_line)\n",
    "\n",
    "        # Close the file object\n",
    "        csvfile.close()\n",
    "\n",
    "else:\n",
    "    # CREATE CSV FILE\n",
    "    csvfile = open(filename,'w',encoding='utf-8')\n",
    "    csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "    # ADD THE HEADER TO CSV FILE\n",
    "    csvfile_writer.writerow([\"question\",\"answer\"])\n",
    "\n",
    "    # FOR EACH PROMPT\n",
    "    for prompt in root.findall('prompt'):\n",
    "        if(prompt):\n",
    "            # EXTRACT DETAILS  \n",
    "            question = prompt.find(\"question\")\n",
    "            answer = prompt.find(\"question_answer\")\n",
    "            csv_line = [question.text, \"<question_answer>\" + answer.text + \"</question_answer>\"]\n",
    "            # ADD A NEW ROW TO CSV FILE\n",
    "            csvfile_writer.writerow(csv_line)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebded6-6211-48de-9cf0-f08d32beb988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Alternative if you need more questions/context\n",
    "from xml.etree import ElementTree\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_to_xml_method1(data_str):\n",
    "    root = ET.fromstring(data_str)\n",
    "    return root\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        {question}\n",
    "        - Return each response in <prompt> </prompt> XML tags.\n",
    "        - Inside <promp> tag, return the question inside <question></question> XML tags.\n",
    "        - Inside <promp> tag, return the answer inside <question_answer> </question_answer> XML tags.\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return only the result in XML format\n",
    "        \n",
    "        \n",
    "        Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "filename = 'qsdata.csv'\n",
    "\n",
    "query = \"Generate 10 questions and answers and return the result in XML format.\"\n",
    "resultset = []\n",
    "\n",
    "for token in token_text_list:\n",
    "    # Pass in values to the input variables\n",
    "    prompt = PROMPT.format(question=query, \n",
    "                                     context=token\n",
    "         )\n",
    "    response = llm(prompt)\n",
    "    resultset.append(response.split(\"\\n\",2)[2])\n",
    "    \n",
    "\n",
    "# PARSE XML\n",
    "# remove first two lines of response\n",
    "xmlstring = ' '.join([str(elem) for elem in resultset])\n",
    "result = '<prompts>' + xmlstring + '</prompts>'\n",
    "try:\n",
    "    root = convert_to_xml_method1(result)\n",
    "\n",
    "    # check if file exists\n",
    "    # Open our existing CSV file in append mode\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'a',encoding='utf-8') as csvfile:\n",
    "            # Pass this file object to csv.writer()\n",
    "            # and get a writer object\n",
    "            csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "            for prompt in root.findall('prompt'):\n",
    "                if(prompt):\n",
    "                    # EXTRACT DETAILS  \n",
    "                    question = prompt.find(\"question\")\n",
    "                    answer = prompt.find(\"question_answer\")\n",
    "                    csv_line = [question.text, \"<question_answer>\" + answer.text + \"</question_answer>\"]\n",
    "                    # ADD A NEW ROW TO CSV FILE\n",
    "                    csvfile_writer.writerow(csv_line)\n",
    "\n",
    "            # Close the file object\n",
    "            csvfile.close()\n",
    "\n",
    "    else:\n",
    "        # CREATE CSV FILE\n",
    "        csvfile = open(filename,'w',encoding='utf-8')\n",
    "        csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "        # ADD THE HEADER TO CSV FILE\n",
    "        csvfile_writer.writerow([\"question\",\"answer\"])\n",
    "\n",
    "        # FOR EACH PROMPT\n",
    "        for prompt in root.findall('prompt'):\n",
    "            if(prompt):\n",
    "                # EXTRACT DETAILS  \n",
    "                question = prompt.find(\"question\")\n",
    "                answer = prompt.find(\"question_answer\")\n",
    "                csv_line = [question.text, \"<question_answer>\" + answer.text + \"</question_answer>\"]\n",
    "                # ADD A NEW ROW TO CSV FILE\n",
    "                csvfile_writer.writerow(csv_line)\n",
    "        csvfile.close()\n",
    "except:\n",
    "    print('error occured, continue with next tokensplit')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
